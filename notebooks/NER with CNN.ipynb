{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    __dir__ = '/content/drive/My Drive/Colab Notebooks/nlp_ner_workshop'\n",
    "else:\n",
    "    from os.path import abspath\n",
    "    __dir__ = abspath(\"..\")\n",
    "sys.path.append(__dir__ + '/python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import json\n",
    "import collections\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import TimeDistributed, Dense, Activation, Input, Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_FILE = __dir__ + '/data/0.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of legal documents that have been pre-processed, tokenized and tagged. They're stored in one zip file of JSON files. To save space we'll read the files directly from the zip.\n",
    "Let's see what one of them looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['exhibit', 'n']]\n",
      "[['exhibit', 'n'], ['{num}', 'n']]\n",
      "[['execution', 'b'], ['version', 'b']]\n",
      "[['credit', 'b'], ['agreement', 'b']]\n",
      "[['dated', 'n'], ['as', 'n'], ['of', 'n']]\n",
      "[['april', 'n'], ['{num}', 'n'], ['{num}', 'n']]\n",
      "[['among', 'n']]\n",
      "[['cleco', 'b'], ['mergersub', 'b'], ['inc', 'b'], ['.', 'b'], [',', 'n']]\n",
      "[['as', 'n'], ['initial', 'n'], ['borrower', 'n'], [',', 'n']]\n",
      "[['to', 'n'], ['be', 'n'], ['merged', 'n'], ['with', 'n'], ['and', 'n'], ['into', 'n']]\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(DATA_FILE) as z:\n",
    "    filename = z.filelist[0]\n",
    "    with z.open(filename) as f:\n",
    "        content = json.load(f)\n",
    "        print(*content[:10], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the document is split into lines (sequences) made of (token, tag) pairs. For instance, the tokens 'execution' and 'version' (3rd line) are tagged with 'b' indicating that they are **bold**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll concatenate all of these sequences into one big dataset.  \n",
    "**NOTE**: Due to memory/time constraints we will limit the number of documents we use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_zip_file(input_file, read_limit=1000):\n",
    "    with ZipFile(input_file) as z:\n",
    "        all_x = []\n",
    "        for fname in z.filelist[:read_limit]:\n",
    "            with z.open(fname) as f:\n",
    "                all_x += json.load(f)\n",
    "    return all_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = read_json_zip_file(DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our RNN will work on sequences, we need to determine the maximum sequence length we'll use. The length of the sequence will impact the memory our model uses and the time it takes to train it.   \n",
    "We'll choose a maximum sequence length of 256 so that almost 98% of our data will be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sequences(sequences, max_length=256):\n",
    "    lengths = [len(x) for x in sequences]\n",
    "    print(f'Maximum length: {max(lengths)}')\n",
    "    print(f'Minimum length: {min(lengths)}')\n",
    "    print(f'Average length: {sum(lengths)/len(lengths)}')\n",
    "    \n",
    "    short_sequences = [s for s in sequences if len(s) <= max_length]\n",
    "    print(f'% of short sequences: {100 * len(short_sequences)/len(sequences)}')\n",
    "    X = [[c[0] for c in x] for x in short_sequences]\n",
    "    y = [[c[1] for c in y] for y in short_sequences]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 4265\n",
      "Minimum length: 1\n",
      "Average length: 35.93670324570742\n",
      "% of short sequences: 97.88135014877297\n"
     ]
    }
   ],
   "source": [
    "max_length = 256\n",
    "X, y = filter_sequences(sequences, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word sequences ready we need to represent them in a machine readable (=numerical) format. To do that, we'll merge all of the tokens into one big corpus and assign each token a unique number.  \n",
    "**NOTE**: We will remove words that appear less than `min_token_frequency` since we can't learn much from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_token_frequency = 2\n",
    "corpus = (token for sequence in X for token in sequence)\n",
    "\n",
    "## Reserve the first two indexes for padding and uknowns\n",
    "index2token = [\"{pad}\", \"{unk}\"] + [token for token, count in collections.Counter(corpus).items() if count >= min_token_frequency]\n",
    "token2index = collections.defaultdict(lambda: 1, {token: index for index, token in enumerate(index2token)})\n",
    "\n",
    "index2label = [\"{pad}\"] + list(set([label for target in y for label in target]))\n",
    "label2index = {label: index for index, label in enumerate(index2label)}\n",
    "\n",
    "with open(__dir__+'/model/model_params.json', 'w') as file:\n",
    "    json.dump({\n",
    "        \"word2ind\": dict(token2index),\n",
    "        \"label2ind\": dict(label2index),\n",
    "        \"max_length\": max_length\n",
    "    }, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some stats about our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocabulary size:  23507\n",
      "Label vocabulary size:  6\n",
      "Maximum sequence length:  256\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = max([len(x) for x in X])\n",
    "\n",
    "print('Text vocabulary size: ', len(token2index))\n",
    "print('Label vocabulary size: ', len(label2index))\n",
    "print('Maximum sequence length: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the dictionaries we've just created to encode our data.  \n",
    "**NOTE**: We pad our data so that each sample we feed our network has the same size. (This is not necessarily required but it improves Keras' perfromance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-461f54d012a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mX_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0my_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ugore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     79\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ugore\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_enc = [[token2index[token] for token in sequence] for sequence in X]\n",
    "max_label = len(label2index)\n",
    "y_enc = [[0] * (max_sequence_length - len(target)) + [label2index[label] for label in target] for target in y]\n",
    "y_enc = [to_categorical(target, max_label) for target in y_enc]\n",
    "\n",
    "X_enc = pad_sequences(X_enc, maxlen=max_sequence_length)\n",
    "y_enc = pad_sequences(y_enc, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in preparing the data is to split it into train/test set which we will later feed into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_enc, y_enc, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our model, let's define some parameters which we'll need later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(token2index)\n",
    "tags_size = len(label2index)\n",
    "embedding_size = 128\n",
    "conv_filters = 100\n",
    "conv_kernel_size = 5\n",
    "batch_size = 2048\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our CNN, we're going to use Keras. We'll build it one layer at a time, explaining each layer as we go.  \n",
    "The first layer is the input layer which will be given the `X_train` values we created previously. The shape of the layer is determined by the `max_sequence_length` we set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input = Input(shape=(max_sequence_length,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the embedding layer. This layer takes our numerical representation of tokens and learns a (hopefully) meaningful embedding for each of them. The input dimension for this layer is determined by the size of our vocabulary while the output dimension (embedding size) is chosen arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_embed = Embedding(vocabulary_size, embedding_size, input_length=max_sequence_length)(l_input) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define 1D convolutional layer which defines a filter of size `conv_kernel_size` (also called window size). Only defining one filter would allow the neural network to learn one single feature in the first layer. This might not be sufficient, therefore we will define `conv_filters` filters.  \n",
    "We set `same` padding so the output has the same length as the original input.\n",
    "creates a convolution kernel that is convolved with the layer input over a single spatial dimension to produce a tensor of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_conv = Conv1D(conv_filters, conv_kernel_size, padding='same', activation='relu')(l_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a `Dense` layer with the size of `tags_size` - the number of tags we want to predict, wrapped in a `TimeDistributed` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dense = TimeDistributed(Dense(tags_size))(l_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we add the `Activation` layer with a softmax function to determine the tag of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_active = Activation('softmax')(l_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create and compile the model using categorical crossentropy for our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=l_input, outputs=l_active)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what the model looks like by printing its summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit the model using the training data (this might tke a while depending on your hardware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use several techniques to evaluate the results of our model. The first one is simply to use Keras's built-in `evaluate` function which will return the test loss - lower means better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(f'Model loss: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next metrics we'll look at are the accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes our sequence predictions/true values and returns them as lists \n",
    "def reshape_predictions(yh, pr):\n",
    "    coords = [np.where(yhh > 0)[0][0] for yhh in yh]\n",
    "    yh = [yhh[co:] for yhh, co in zip(yh, coords)]\n",
    "    ypr = [prr[co:] for prr, co in zip(pr, coords)]\n",
    "    fyh = [c for row in yh for c in row]\n",
    "    fpr = [c for row in ypr for c in row]\n",
    "    return fyh, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train).argmax(2)\n",
    "yh = y_train.argmax(2)\n",
    "fyh, fpr = reshape_predictions(yh, y_pred)\n",
    "print('Training accuracy:', accuracy_score(fyh, fpr))\n",
    "print('Training confusion matrix:')\n",
    "print(confusion_matrix(fyh, fpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy is fairly high which is what we want but it might also indicate that our model isn't doing what we think it's doing. The accuracy metric measures the percentage of correct predictions. So, in our case, 95% if our tokens were marked as 'n' and 5% as any other label then if our model predicted 'n' for **all** input it would have an accuracy of 95%.  \n",
    "Looking at the confusion matrix helps us gain insight as to what kind of errors our model makes. The columns of the matrix represent the prediction made by the model while the rows represent the true value - numbers on the diagonal are correct predicitions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the confusion matrix is sometimes difficult when comparing between different models/model parameters especially when there are a lot of classes. Looking at the precision, recall and F1 score might be easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = precision_recall_fscore_support(fyh, fpr)\n",
    "print(f'Precision: {results[0][1:]}')\n",
    "print(f'Recall:    {results[1][1:]}')\n",
    "print(f'F1-Score:  {results[2][1:]}')\n",
    "print(f'Support:   {results[3][1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for label 4 ('n') we get good values across all metrics whereas for other labels we do rather poorly. This doesn't necessarily mean that our model isn't good - it really depends on what and how we're going to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we've only evaluated our models performance on the training data but in most cases we're more interested to know how well our model generalizes and how it performs on the data that it hasn't seen before - our test data.  \n",
    "Let's evaluate the model's performance on the test data the same way we did for the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test).argmax(2)\n",
    "yh_test = y_test.argmax(2)\n",
    "fyh_test, fpr_test = reshape_predictions(yh_test, y_pred_test)\n",
    "print('Training accuracy:', accuracy_score(fyh_test, fpr_test))\n",
    "print('Training confusion matrix:')\n",
    "print(confusion_matrix(fyh_test, fpr_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = precision_recall_fscore_support(fyh_test, fpr_test)\n",
    "print(f'Precision: {results_test[0][1:]}')\n",
    "print(f'Recall:    {results_test[1][1:]}')\n",
    "print(f'F1-Score:  {results_test[2][1:]}')\n",
    "print(f'Support:   {results_test[3][1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results seem similar to the ones we got for the training data which means our model generalizes well.  \n",
    "At this point we can save our model architecture and weights so that we can use it later without having to build and train it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(__dir__+'/model/model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "model.save_weights(__dir__+'/model/model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
