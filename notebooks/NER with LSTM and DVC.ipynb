{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER with LSTM and DVC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "BVZIqG4PfIjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP Using DVC and DAGsHub\n",
        "\n",
        "This notebook demonstrates how you can leverage git and [DVC](https://github.com/iterative/dvc) to easily manage ML experiments, including of course NLP.\n",
        "\n",
        "By using this structure, you can quickly try a lot of different configurations, train a lot of models, and submit the ones that perform best.\n",
        "\n",
        "## Instructions\n",
        "\n",
        "* This notebook is intended to be used in Google Colab.\n",
        "* To get started, we recommend you create a user in https://dagshub.com/user/sign_up\n",
        "* Create a fork of the following repo: https://dagshub.com/Guy/uri_nlp_ner_workshop\n",
        "  * [Link to fork creation screen](https://dagshub.com/repo/fork/19)\n",
        "* If you want to modify the training code, e.g. switch to CNN instead of LSTM, we recommend to clone the repo to your laptop and edit it there. You can then push the modified code back to your repo, and the experiments in this notebook will automatically pull the latest version.\n",
        "* In the **Setup** cell below, fill in your DAGsHub username `dagshub_user`.\n",
        "  * If you leave the `dagshub_user` blank, then the original repo will be cloned instead of your fork. While this is OK, it will mean that you can't push the results of your experiments and be able to resume them if Google disconnects your Colab session (which is likely to happen).\n",
        "  * If you do fill in `dagshub_user`, you will be prompted for your password, and will be able to push the results of experiments back to your git repo.\n",
        "  * Alternatively, you can configure a git remote on your Google Drive, after it's mounted.\n",
        "* The Setup section clones your git repo, mounts your Google Drive, and configures DVC to manage the different versions of your experiments inside a folder in your Google Drive.\n",
        "* The **Experiment configuration** and **Experiment run** sections are the main part - here you can try a lot of different configurations, automatically commiting the result of each experiment to git and saving the resulting model to your Google Drive.\n",
        "* The **Experiment overview** section is meant to run last, when you're choosing the best model. If you chose to create a fork in https://dagshub.com, you can compare metrics more comfortably over there.\n",
        "* When you decide what you want to submit, run the **Submit results** section."
      ]
    },
    {
      "metadata": {
        "id": "C8L3Ro8Bf0Zd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ]
    },
    {
      "metadata": {
        "id": "jPfovdmjzQY4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logging in and cloning your DAGsHub repo\n",
        "### RESTART THE RUNTIME AFTER RUNNING THIS CELL ONCE!"
      ]
    },
    {
      "metadata": {
        "id": "BvA3Q53KWhcO",
        "colab_type": "code",
        "outputId": "2af7cf07-81b9-445a-cc09-47b488a0b5e1",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "### RESTART THE RUNTIME AFTER RUNNING THIS CELL ONCE!\n",
        "from getpass import getpass\n",
        "dagshub_user = \"\" #@param {type:\"string\"}\n",
        "user_email = \"someone@somewhere.org\" #@param {type:\"string\"} \n",
        "!git config --global user.name {dagshub_user}\n",
        "!git config --global user.email {user_email}\n",
        "if dagshub_user:\n",
        "  dagshub_pass = getpass('DAGsHub password: ')\n",
        "!git clone https://{dagshub_user + ':' + dagshub_pass + '@' if dagshub_user else ''}dagshub.com/{dagshub_user if dagshub_user else 'Guy'}/uri_nlp_ner_workshop.git # Change this to the URL for your fork of Uri's repo\n",
        "dagshub_pass = None\n",
        "!pip install -q -r uri_nlp_ner_workshop/requirements.txt\n",
        "!pip install -q dvc\n",
        "!pip uninstall -yq enum34\n",
        "### RESTART THE RUNTIME AFTER RUNNING THIS CELL ONCE!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Git password: ··········\n",
            "Cloning into 'uri_nlp_ner_workshop'...\n",
            "remote: Enumerating objects: 1179, done.\u001b[K\n",
            "remote: Counting objects: 100% (1179/1179), done.\u001b[K\n",
            "remote: Compressing objects: 100% (957/957), done.\u001b[K\n",
            "remote: Total 1179 (delta 222), reused 1134 (delta 202)\u001b[K\n",
            "Receiving objects: 100% (1179/1179), 6.25 MiB | 21.21 MiB/s, done.\n",
            "Resolving deltas: 100% (222/222), done.\n",
            "\u001b[K    100% |████████████████████████████████| 17.3MB 1.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 3.2MB 12.8MB/s \n",
            "\u001b[31mtorchvision 0.2.1 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mthinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mpymc3 3.6 has requirement joblib<0.13.0, but you'll have joblib 0.13.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mfeaturetools 0.4.1 has requirement pandas>=0.23.0, but you'll have pandas 0.22.0 which is incompatible.\u001b[0m\n",
            "\u001b[31malbumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.8 which is incompatible.\u001b[0m\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 6.7MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 450kB 9.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 51kB 20.8MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 102kB 29.5MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 71kB 25.2MB/s \n",
            "\u001b[K    100% |████████████████████████████████| 870kB 20.9MB/s \n",
            "\u001b[?25h  Building wheel for jsonpath-rw (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for configobj (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for nanotime (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Building wheel for zc.lockfile (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tKttwg_p5lLO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**RESTART THE RUNTIME NOW!**"
      ]
    },
    {
      "metadata": {
        "id": "-CGsJlouzNL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Google Drive to be DVC remote for the project\n",
        "### One time setup - run this once after restarting the runtime (after running the first cell and restaring the runtime)"
      ]
    },
    {
      "metadata": {
        "id": "mAlujcvCWky_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run this cell once after restarting the runtime\n",
        "import os\n",
        "os.chdir('uri_nlp_ner_workshop')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7mrwvJ3YmJYs",
        "colab_type": "code",
        "outputId": "f124a12a-423b-468b-8a3d-207a52e5cade",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#Set up DVC\n",
        "!mkdir -p '/content/gdrive/My Drive/nlp-workshop/dvc-cache'\n",
        "!dvc remote add --local gdrive-remote '/content/gdrive/My Drive/nlp-workshop-dvc-cache'\n",
        "!dvc config --local core.remote gdrive-remote\n",
        "!dvc pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[0mPreparing to download data from '/content/gdrive/My Drive/nlp-workshop-dvc-cache'\n",
            "Preparing to collect status from /content/gdrive/My Drive/nlp-workshop-dvc-cache\n",
            "\u001b[K[##############################] 100% Collecting information\n",
            "\u001b[K[##############################] 100% Analysing status.\n",
            "\u001b[K(1/2): [##############################] 100% ../model/model_arch.json\n",
            "\u001b[K(2/2): [##############################] 100% 0.zip\n",
            "Checking out '{'scheme': 'local', 'path': '/content/uri_nlp_ner_workshop/data/0.zip'}' with cache 'b579e7168811a71560bd97b6ef954413'.\n",
            "Checking out '{'scheme': 'local', 'path': '/content/uri_nlp_ner_workshop/model/model_arch.json'}' with cache 'c3d0b037aa05579ed0c8e74098a20d49'.\n",
            "\u001b[33mWarning\u001b[39m: Cache '3a44a8242693b363b9076fc637972b99' not found. File '{'scheme': 'local', 'path': '/content/uri_nlp_ner_workshop/model/model_weights.h5'}' won't be created.\n",
            "\u001b[33mWarning\u001b[39m: Cache '003898446d853e2e523f40ed118e7f3b' not found. File '{'scheme': 'local', 'path': '/content/uri_nlp_ner_workshop/model/learn-stdout.txt'}' won't be created.\n",
            "\u001b[0m\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y3Z7dweXhkpB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment configuration\n",
        "To set up a new experiment, edit the fields on the right, then execute the cell.\n",
        "\n",
        "It will save the experiment params into train_params.yaml, as well as create a new git branch for your experiment."
      ]
    },
    {
      "metadata": {
        "id": "vECZJJDktThC",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "311350bf-bf85-40ff-b627-cba64d2242d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Experiment Hyperparameters\n",
        "experiment_name = \"second-experiment\" #@param {type: \"string\"}\n",
        "#@markdown * WARNING: experiment name must be valid alpha or numeric or dash(-_) or dot characters.\n",
        "!git fetch\n",
        "!git checkout -b {experiment_name} origin/master\n",
        "\n",
        "read_limit = 2000 #@param {type:\"integer\"}\n",
        "max_sentence_size = 64 #@param {type:\"integer\"}\n",
        "test_size = 0.1 #@param {type:\"number\"}\n",
        "min_word_freq = 2 #@param {type:\"integer\"}\n",
        "batch_size = 1024 #@param {type:\"integer\"}\n",
        "epochs = 1 #@param {type:\"integer\"}\n",
        "embedding_size = 128 #@param {type:\"integer\"}\n",
        "lstm_size = 32 #@param {type:\"integer\"}\n",
        "dropout = 0.25 #@param {type:\"number\"}\n",
        "out_dir = '../model' #@param {type:\"string\"}\n",
        "\n",
        "train_params = {\n",
        "    \"read_limit\": read_limit,\n",
        "    \"max_sentence_size\": max_sentence_size,\n",
        "    \"test_size\": test_size,\n",
        "    \"min_word_freq\": min_word_freq,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs,\n",
        "    \"embedding_size\": embedding_size,\n",
        "    \"lstm_size\": lstm_size,\n",
        "    \"dropout\": dropout,\n",
        "    \"out_dir\": out_dir,\n",
        "}\n",
        "\n",
        "import yaml\n",
        "with open('python/train_params.yaml', 'w') as f:\n",
        "  yaml.dump(train_params, f, default_flow_style=False)\n",
        "  \n",
        "!git add python/train_params.yaml\n",
        "!git diff\n",
        "!git commit -m \"Configured parameters for experiment {experiment_name}\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'second-experiment' set up to track remote branch 'master' from 'origin'.\n",
            "Switched to a new branch 'second-experiment'\n",
            "[second-experiment 23daf17] Configured parameters for experiment second-experiment\n",
            " 1 file changed, 6 insertions(+), 6 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YMm9lMcRvZu-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment run\n",
        "This trains the model and records metrics"
      ]
    },
    {
      "metadata": {
        "id": "-xivLbAUesWn",
        "colab_type": "code",
        "outputId": "1723c9c2-d474-4059-babd-14030270695a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7891
        }
      },
      "cell_type": "code",
      "source": [
        "!dvc repro python/learn.dvc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32mStage 'data/0.zip.dvc' didn't change.\u001b[39m\n",
            "\u001b[33mWarning\u001b[39m: Dependency 'python/train_params.yaml' of 'python/learn.dvc' changed.\n",
            "\u001b[33mWarning\u001b[39m: Output 'model/model_params.json' of 'python/learn.dvc' changed.\n",
            "\u001b[33mStage 'python/learn.dvc' changed.\u001b[39m\n",
            "Reproducing 'python/learn.dvc'\n",
            "Running command:\n",
            "\tpython3 style_learn.py 2>&1 | tee ../model/learn-stdout.txt\n",
            "Using TensorFlow backend.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2019-03-04 00:39:52.029935: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-03-04 00:39:52.030200: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1f52840 executing computations on platform Host. Devices:\n",
            "2019-03-04 00:39:52.030229: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "Train params:\n",
            "{'batch_size': 1024, 'dropout': 0.25, 'embedding_size': 128, 'epochs': 1, 'lstm_size': 32, 'max_sentence_size': 64, 'min_word_freq': 2, 'out_dir': '../model', 'read_limit': 2000, 'test_size': 0.1}\n",
            "Input sequence length range:  4265 1\n",
            "# of short sequences: 385084/472599 \n",
            "Vocabulary size: 23384 6\n",
            "Maximum sequence length: 64\n",
            "Training and testing tensor shapes: (346575, 64) (38509, 64) (346575, 64, 6) (38509, 64, 6)\n",
            "Train on 346575 samples, validate on 38509 samples\n",
            "Epoch 1/1\n",
            "\n",
            "  1024/346575 [..............................] - ETA: 8:27 - loss: 1.7898\n",
            "  2048/346575 [..............................] - ETA: 6:05 - loss: 1.7848\n",
            "  3072/346575 [..............................] - ETA: 5:16 - loss: 1.7785\n",
            "  4096/346575 [..............................] - ETA: 4:52 - loss: 1.7719\n",
            "  5120/346575 [..............................] - ETA: 4:38 - loss: 1.7648\n",
            "  6144/346575 [..............................] - ETA: 4:27 - loss: 1.7573\n",
            "  7168/346575 [..............................] - ETA: 4:20 - loss: 1.7495\n",
            "  8192/346575 [..............................] - ETA: 4:15 - loss: 1.7411\n",
            "  9216/346575 [..............................] - ETA: 4:10 - loss: 1.7321\n",
            " 10240/346575 [..............................] - ETA: 4:06 - loss: 1.7227\n",
            " 11264/346575 [..............................] - ETA: 4:03 - loss: 1.7127\n",
            " 12288/346575 [>.............................] - ETA: 4:01 - loss: 1.7018\n",
            " 13312/346575 [>.............................] - ETA: 3:59 - loss: 1.6900\n",
            " 14336/346575 [>.............................] - ETA: 3:57 - loss: 1.6776\n",
            " 15360/346575 [>.............................] - ETA: 3:55 - loss: 1.6640\n",
            " 16384/346575 [>.............................] - ETA: 3:54 - loss: 1.6489\n",
            " 17408/346575 [>.............................] - ETA: 3:52 - loss: 1.6326\n",
            " 18432/346575 [>.............................] - ETA: 3:51 - loss: 1.6147\n",
            " 19456/346575 [>.............................] - ETA: 3:50 - loss: 1.5959\n",
            " 20480/346575 [>.............................] - ETA: 3:48 - loss: 1.5755\n",
            " 21504/346575 [>.............................] - ETA: 3:47 - loss: 1.5544\n",
            " 22528/346575 [>.............................] - ETA: 3:46 - loss: 1.5313\n",
            " 23552/346575 [=>............................] - ETA: 3:45 - loss: 1.5059\n",
            " 24576/346575 [=>............................] - ETA: 3:44 - loss: 1.4786\n",
            " 25600/346575 [=>............................] - ETA: 3:43 - loss: 1.4518\n",
            " 26624/346575 [=>............................] - ETA: 3:42 - loss: 1.4249\n",
            " 27648/346575 [=>............................] - ETA: 3:41 - loss: 1.3975\n",
            " 28672/346575 [=>............................] - ETA: 3:40 - loss: 1.3702\n",
            " 29696/346575 [=>............................] - ETA: 3:39 - loss: 1.3451\n",
            " 30720/346575 [=>............................] - ETA: 3:38 - loss: 1.3207\n",
            " 31744/346575 [=>............................] - ETA: 3:37 - loss: 1.2964\n",
            " 32768/346575 [=>............................] - ETA: 3:36 - loss: 1.2732\n",
            " 33792/346575 [=>............................] - ETA: 3:35 - loss: 1.2506\n",
            " 34816/346575 [==>...........................] - ETA: 3:34 - loss: 1.2282\n",
            " 35840/346575 [==>...........................] - ETA: 3:33 - loss: 1.2066\n",
            " 36864/346575 [==>...........................] - ETA: 3:32 - loss: 1.1866\n",
            " 37888/346575 [==>...........................] - ETA: 3:31 - loss: 1.1701\n",
            " 38912/346575 [==>...........................] - ETA: 3:30 - loss: 1.1535\n",
            " 39936/346575 [==>...........................] - ETA: 3:29 - loss: 1.1353\n",
            " 40960/346575 [==>...........................] - ETA: 3:28 - loss: 1.1196\n",
            " 41984/346575 [==>...........................] - ETA: 3:28 - loss: 1.1052\n",
            " 43008/346575 [==>...........................] - ETA: 3:27 - loss: 1.0904\n",
            " 44032/346575 [==>...........................] - ETA: 3:26 - loss: 1.0753\n",
            " 45056/346575 [==>...........................] - ETA: 3:25 - loss: 1.0617\n",
            " 46080/346575 [==>...........................] - ETA: 3:24 - loss: 1.0480\n",
            " 47104/346575 [===>..........................] - ETA: 3:23 - loss: 1.0345\n",
            " 48128/346575 [===>..........................] - ETA: 3:22 - loss: 1.0224\n",
            " 49152/346575 [===>..........................] - ETA: 3:21 - loss: 1.0105\n",
            " 50176/346575 [===>..........................] - ETA: 3:21 - loss: 1.0002\n",
            " 51200/346575 [===>..........................] - ETA: 3:20 - loss: 0.9903\n",
            " 52224/346575 [===>..........................] - ETA: 3:19 - loss: 0.9791\n",
            " 53248/346575 [===>..........................] - ETA: 3:18 - loss: 0.9692\n",
            " 54272/346575 [===>..........................] - ETA: 3:17 - loss: 0.9597\n",
            " 55296/346575 [===>..........................] - ETA: 3:16 - loss: 0.9510\n",
            " 56320/346575 [===>..........................] - ETA: 3:16 - loss: 0.9414\n",
            " 57344/346575 [===>..........................] - ETA: 3:15 - loss: 0.9329\n",
            " 58368/346575 [====>.........................] - ETA: 3:14 - loss: 0.9244\n",
            " 59392/346575 [====>.........................] - ETA: 3:13 - loss: 0.9157\n",
            " 60416/346575 [====>.........................] - ETA: 3:12 - loss: 0.9103\n",
            " 61440/346575 [====>.........................] - ETA: 3:11 - loss: 0.9027\n",
            " 62464/346575 [====>.........................] - ETA: 3:11 - loss: 0.8968\n",
            " 63488/346575 [====>.........................] - ETA: 3:10 - loss: 0.8901\n",
            " 64512/346575 [====>.........................] - ETA: 3:09 - loss: 0.8840\n",
            " 65536/346575 [====>.........................] - ETA: 3:08 - loss: 0.8772\n",
            " 66560/346575 [====>.........................] - ETA: 3:07 - loss: 0.8713\n",
            " 67584/346575 [====>.........................] - ETA: 3:07 - loss: 0.8645\n",
            " 68608/346575 [====>.........................] - ETA: 3:06 - loss: 0.8580\n",
            " 69632/346575 [=====>........................] - ETA: 3:05 - loss: 0.8513\n",
            " 70656/346575 [=====>........................] - ETA: 3:04 - loss: 0.8457\n",
            " 71680/346575 [=====>........................] - ETA: 3:03 - loss: 0.8395\n",
            " 72704/346575 [=====>........................] - ETA: 3:03 - loss: 0.8340\n",
            " 73728/346575 [=====>........................] - ETA: 3:02 - loss: 0.8290\n",
            " 74752/346575 [=====>........................] - ETA: 3:01 - loss: 0.8239\n",
            " 75776/346575 [=====>........................] - ETA: 3:01 - loss: 0.8192\n",
            " 76800/346575 [=====>........................] - ETA: 3:00 - loss: 0.8139\n",
            " 77824/346575 [=====>........................] - ETA: 2:59 - loss: 0.8091\n",
            " 78848/346575 [=====>........................] - ETA: 2:58 - loss: 0.8038\n",
            " 79872/346575 [=====>........................] - ETA: 2:58 - loss: 0.7984\n",
            " 80896/346575 [======>.......................] - ETA: 2:57 - loss: 0.7945\n",
            " 81920/346575 [======>.......................] - ETA: 2:56 - loss: 0.7897\n",
            " 82944/346575 [======>.......................] - ETA: 2:55 - loss: 0.7853\n",
            " 83968/346575 [======>.......................] - ETA: 2:55 - loss: 0.7806\n",
            " 84992/346575 [======>.......................] - ETA: 2:54 - loss: 0.7759\n",
            " 86016/346575 [======>.......................] - ETA: 2:53 - loss: 0.7718\n",
            " 87040/346575 [======>.......................] - ETA: 2:52 - loss: 0.7676\n",
            " 88064/346575 [======>.......................] - ETA: 2:52 - loss: 0.7639\n",
            " 89088/346575 [======>.......................] - ETA: 2:51 - loss: 0.7596\n",
            " 90112/346575 [======>.......................] - ETA: 2:50 - loss: 0.7563\n",
            " 91136/346575 [======>.......................] - ETA: 2:50 - loss: 0.7525\n",
            " 92160/346575 [======>.......................] - ETA: 2:49 - loss: 0.7490\n",
            " 93184/346575 [=======>......................] - ETA: 2:48 - loss: 0.7459\n",
            " 94208/346575 [=======>......................] - ETA: 2:48 - loss: 0.7424\n",
            " 95232/346575 [=======>......................] - ETA: 2:47 - loss: 0.7393\n",
            " 96256/346575 [=======>......................] - ETA: 2:46 - loss: 0.7356\n",
            " 97280/346575 [=======>......................] - ETA: 2:46 - loss: 0.7321\n",
            " 98304/346575 [=======>......................] - ETA: 2:45 - loss: 0.7286\n",
            " 99328/346575 [=======>......................] - ETA: 2:44 - loss: 0.7256\n",
            "100352/346575 [=======>......................] - ETA: 2:43 - loss: 0.7219\n",
            "101376/346575 [=======>......................] - ETA: 2:43 - loss: 0.7184\n",
            "102400/346575 [=======>......................] - ETA: 2:42 - loss: 0.7150\n",
            "103424/346575 [=======>......................] - ETA: 2:41 - loss: 0.7121\n",
            "104448/346575 [========>.....................] - ETA: 2:41 - loss: 0.7090\n",
            "105472/346575 [========>.....................] - ETA: 2:40 - loss: 0.7060\n",
            "106496/346575 [========>.....................] - ETA: 2:39 - loss: 0.7036\n",
            "107520/346575 [========>.....................] - ETA: 2:39 - loss: 0.7007\n",
            "108544/346575 [========>.....................] - ETA: 2:38 - loss: 0.6978\n",
            "109568/346575 [========>.....................] - ETA: 2:37 - loss: 0.6948\n",
            "110592/346575 [========>.....................] - ETA: 2:37 - loss: 0.6924\n",
            "111616/346575 [========>.....................] - ETA: 2:36 - loss: 0.6896\n",
            "112640/346575 [========>.....................] - ETA: 2:35 - loss: 0.6872\n",
            "113664/346575 [========>.....................] - ETA: 2:35 - loss: 0.6846\n",
            "114688/346575 [========>.....................] - ETA: 2:34 - loss: 0.6823\n",
            "115712/346575 [=========>....................] - ETA: 2:33 - loss: 0.6804\n",
            "116736/346575 [=========>....................] - ETA: 2:33 - loss: 0.6776\n",
            "117760/346575 [=========>....................] - ETA: 2:32 - loss: 0.6748\n",
            "118784/346575 [=========>....................] - ETA: 2:31 - loss: 0.6722\n",
            "119808/346575 [=========>....................] - ETA: 2:30 - loss: 0.6702\n",
            "120832/346575 [=========>....................] - ETA: 2:30 - loss: 0.6678\n",
            "121856/346575 [=========>....................] - ETA: 2:29 - loss: 0.6656\n",
            "122880/346575 [=========>....................] - ETA: 2:28 - loss: 0.6630\n",
            "123904/346575 [=========>....................] - ETA: 2:28 - loss: 0.6608\n",
            "124928/346575 [=========>....................] - ETA: 2:27 - loss: 0.6588\n",
            "125952/346575 [=========>....................] - ETA: 2:26 - loss: 0.6564\n",
            "126976/346575 [=========>....................] - ETA: 2:26 - loss: 0.6542\n",
            "128000/346575 [==========>...................] - ETA: 2:25 - loss: 0.6525\n",
            "129024/346575 [==========>...................] - ETA: 2:24 - loss: 0.6503\n",
            "130048/346575 [==========>...................] - ETA: 2:24 - loss: 0.6482\n",
            "131072/346575 [==========>...................] - ETA: 2:23 - loss: 0.6458\n",
            "132096/346575 [==========>...................] - ETA: 2:22 - loss: 0.6438\n",
            "133120/346575 [==========>...................] - ETA: 2:21 - loss: 0.6417\n",
            "134144/346575 [==========>...................] - ETA: 2:21 - loss: 0.6394\n",
            "135168/346575 [==========>...................] - ETA: 2:20 - loss: 0.6373\n",
            "136192/346575 [==========>...................] - ETA: 2:19 - loss: 0.6363\n",
            "137216/346575 [==========>...................] - ETA: 2:19 - loss: 0.6347\n",
            "138240/346575 [==========>...................] - ETA: 2:18 - loss: 0.6327\n",
            "139264/346575 [===========>..................] - ETA: 2:17 - loss: 0.6308\n",
            "140288/346575 [===========>..................] - ETA: 2:17 - loss: 0.6294\n",
            "141312/346575 [===========>..................] - ETA: 2:16 - loss: 0.6272\n",
            "142336/346575 [===========>..................] - ETA: 2:15 - loss: 0.6255\n",
            "143360/346575 [===========>..................] - ETA: 2:15 - loss: 0.6237\n",
            "144384/346575 [===========>..................] - ETA: 2:14 - loss: 0.6218\n",
            "145408/346575 [===========>..................] - ETA: 2:13 - loss: 0.6206\n",
            "146432/346575 [===========>..................] - ETA: 2:12 - loss: 0.6187\n",
            "147456/346575 [===========>..................] - ETA: 2:12 - loss: 0.6171\n",
            "148480/346575 [===========>..................] - ETA: 2:11 - loss: 0.6158\n",
            "149504/346575 [===========>..................] - ETA: 2:10 - loss: 0.6144\n",
            "150528/346575 [============>.................] - ETA: 2:10 - loss: 0.6127\n",
            "151552/346575 [============>.................] - ETA: 2:09 - loss: 0.6110\n",
            "152576/346575 [============>.................] - ETA: 2:08 - loss: 0.6095\n",
            "153600/346575 [============>.................] - ETA: 2:08 - loss: 0.6086\n",
            "154624/346575 [============>.................] - ETA: 2:07 - loss: 0.6069\n",
            "155648/346575 [============>.................] - ETA: 2:06 - loss: 0.6052\n",
            "156672/346575 [============>.................] - ETA: 2:06 - loss: 0.6039\n",
            "157696/346575 [============>.................] - ETA: 2:05 - loss: 0.6023\n",
            "158720/346575 [============>.................] - ETA: 2:04 - loss: 0.6011\n",
            "159744/346575 [============>.................] - ETA: 2:04 - loss: 0.5998\n",
            "160768/346575 [============>.................] - ETA: 2:03 - loss: 0.5982\n",
            "161792/346575 [=============>................] - ETA: 2:02 - loss: 0.5968\n",
            "162816/346575 [=============>................] - ETA: 2:01 - loss: 0.5953\n",
            "163840/346575 [=============>................] - ETA: 2:01 - loss: 0.5941\n",
            "164864/346575 [=============>................] - ETA: 2:00 - loss: 0.5927\n",
            "165888/346575 [=============>................] - ETA: 1:59 - loss: 0.5913\n",
            "166912/346575 [=============>................] - ETA: 1:59 - loss: 0.5900\n",
            "167936/346575 [=============>................] - ETA: 1:58 - loss: 0.5887\n",
            "168960/346575 [=============>................] - ETA: 1:57 - loss: 0.5876\n",
            "169984/346575 [=============>................] - ETA: 1:57 - loss: 0.5865\n",
            "171008/346575 [=============>................] - ETA: 1:56 - loss: 0.5851\n",
            "172032/346575 [=============>................] - ETA: 1:55 - loss: 0.5839\n",
            "173056/346575 [=============>................] - ETA: 1:55 - loss: 0.5826\n",
            "174080/346575 [==============>...............] - ETA: 1:54 - loss: 0.5810\n",
            "175104/346575 [==============>...............] - ETA: 1:53 - loss: 0.5798\n",
            "176128/346575 [==============>...............] - ETA: 1:53 - loss: 0.5784\n",
            "177152/346575 [==============>...............] - ETA: 1:52 - loss: 0.5772\n",
            "178176/346575 [==============>...............] - ETA: 1:51 - loss: 0.5761\n",
            "179200/346575 [==============>...............] - ETA: 1:51 - loss: 0.5748\n",
            "180224/346575 [==============>...............] - ETA: 1:50 - loss: 0.5736\n",
            "181248/346575 [==============>...............] - ETA: 1:49 - loss: 0.5724\n",
            "182272/346575 [==============>...............] - ETA: 1:49 - loss: 0.5713\n",
            "183296/346575 [==============>...............] - ETA: 1:48 - loss: 0.5699\n",
            "184320/346575 [==============>...............] - ETA: 1:47 - loss: 0.5685\n",
            "185344/346575 [===============>..............] - ETA: 1:47 - loss: 0.5672\n",
            "186368/346575 [===============>..............] - ETA: 1:46 - loss: 0.5660\n",
            "187392/346575 [===============>..............] - ETA: 1:45 - loss: 0.5648\n",
            "188416/346575 [===============>..............] - ETA: 1:44 - loss: 0.5638\n",
            "189440/346575 [===============>..............] - ETA: 1:44 - loss: 0.5629\n",
            "190464/346575 [===============>..............] - ETA: 1:43 - loss: 0.5619\n",
            "191488/346575 [===============>..............] - ETA: 1:42 - loss: 0.5608\n",
            "192512/346575 [===============>..............] - ETA: 1:42 - loss: 0.5594\n",
            "193536/346575 [===============>..............] - ETA: 1:41 - loss: 0.5584\n",
            "194560/346575 [===============>..............] - ETA: 1:40 - loss: 0.5572\n",
            "195584/346575 [===============>..............] - ETA: 1:40 - loss: 0.5560\n",
            "196608/346575 [================>.............] - ETA: 1:39 - loss: 0.5553\n",
            "197632/346575 [================>.............] - ETA: 1:38 - loss: 0.5543\n",
            "198656/346575 [================>.............] - ETA: 1:38 - loss: 0.5532\n",
            "199680/346575 [================>.............] - ETA: 1:37 - loss: 0.5522\n",
            "200704/346575 [================>.............] - ETA: 1:36 - loss: 0.5514\n",
            "201728/346575 [================>.............] - ETA: 1:36 - loss: 0.5504\n",
            "202752/346575 [================>.............] - ETA: 1:35 - loss: 0.5493\n",
            "203776/346575 [================>.............] - ETA: 1:34 - loss: 0.5485\n",
            "204800/346575 [================>.............] - ETA: 1:34 - loss: 0.5476\n",
            "205824/346575 [================>.............] - ETA: 1:33 - loss: 0.5467\n",
            "206848/346575 [================>.............] - ETA: 1:32 - loss: 0.5457\n",
            "207872/346575 [================>.............] - ETA: 1:32 - loss: 0.5449\n",
            "208896/346575 [=================>............] - ETA: 1:31 - loss: 0.5439\n",
            "209920/346575 [=================>............] - ETA: 1:30 - loss: 0.5429\n",
            "210944/346575 [=================>............] - ETA: 1:30 - loss: 0.5418\n",
            "211968/346575 [=================>............] - ETA: 1:29 - loss: 0.5408\n",
            "212992/346575 [=================>............] - ETA: 1:28 - loss: 0.5400\n",
            "214016/346575 [=================>............] - ETA: 1:28 - loss: 0.5392\n",
            "215040/346575 [=================>............] - ETA: 1:27 - loss: 0.5383\n",
            "216064/346575 [=================>............] - ETA: 1:26 - loss: 0.5376\n",
            "217088/346575 [=================>............] - ETA: 1:25 - loss: 0.5365\n",
            "218112/346575 [=================>............] - ETA: 1:25 - loss: 0.5355\n",
            "219136/346575 [=================>............] - ETA: 1:24 - loss: 0.5346\n",
            "220160/346575 [==================>...........] - ETA: 1:23 - loss: 0.5336\n",
            "221184/346575 [==================>...........] - ETA: 1:23 - loss: 0.5326\n",
            "222208/346575 [==================>...........] - ETA: 1:22 - loss: 0.5314\n",
            "223232/346575 [==================>...........] - ETA: 1:21 - loss: 0.5305\n",
            "224256/346575 [==================>...........] - ETA: 1:21 - loss: 0.5295\n",
            "225280/346575 [==================>...........] - ETA: 1:20 - loss: 0.5287\n",
            "226304/346575 [==================>...........] - ETA: 1:19 - loss: 0.5278\n",
            "227328/346575 [==================>...........] - ETA: 1:19 - loss: 0.5271\n",
            "228352/346575 [==================>...........] - ETA: 1:18 - loss: 0.5263\n",
            "229376/346575 [==================>...........] - ETA: 1:17 - loss: 0.5254\n",
            "230400/346575 [==================>...........] - ETA: 1:17 - loss: 0.5246\n",
            "231424/346575 [===================>..........] - ETA: 1:16 - loss: 0.5240\n",
            "232448/346575 [===================>..........] - ETA: 1:15 - loss: 0.5234\n",
            "233472/346575 [===================>..........] - ETA: 1:15 - loss: 0.5225\n",
            "234496/346575 [===================>..........] - ETA: 1:14 - loss: 0.5217\n",
            "235520/346575 [===================>..........] - ETA: 1:13 - loss: 0.5209\n",
            "236544/346575 [===================>..........] - ETA: 1:13 - loss: 0.5200\n",
            "237568/346575 [===================>..........] - ETA: 1:12 - loss: 0.5192\n",
            "238592/346575 [===================>..........] - ETA: 1:11 - loss: 0.5181\n",
            "239616/346575 [===================>..........] - ETA: 1:11 - loss: 0.5175\n",
            "240640/346575 [===================>..........] - ETA: 1:10 - loss: 0.5167\n",
            "241664/346575 [===================>..........] - ETA: 1:09 - loss: 0.5158\n",
            "242688/346575 [====================>.........] - ETA: 1:08 - loss: 0.5152\n",
            "243712/346575 [====================>.........] - ETA: 1:08 - loss: 0.5145\n",
            "244736/346575 [====================>.........] - ETA: 1:07 - loss: 0.5136\n",
            "245760/346575 [====================>.........] - ETA: 1:06 - loss: 0.5129\n",
            "246784/346575 [====================>.........] - ETA: 1:06 - loss: 0.5122\n",
            "247808/346575 [====================>.........] - ETA: 1:05 - loss: 0.5115\n",
            "248832/346575 [====================>.........] - ETA: 1:04 - loss: 0.5108\n",
            "249856/346575 [====================>.........] - ETA: 1:04 - loss: 0.5100\n",
            "250880/346575 [====================>.........] - ETA: 1:03 - loss: 0.5092\n",
            "251904/346575 [====================>.........] - ETA: 1:02 - loss: 0.5084\n",
            "252928/346575 [====================>.........] - ETA: 1:02 - loss: 0.5078\n",
            "253952/346575 [====================>.........] - ETA: 1:01 - loss: 0.5069\n",
            "254976/346575 [=====================>........] - ETA: 1:00 - loss: 0.5062\n",
            "256000/346575 [=====================>........] - ETA: 1:00 - loss: 0.5055\n",
            "257024/346575 [=====================>........] - ETA: 59s - loss: 0.5046 \n",
            "258048/346575 [=====================>........] - ETA: 58s - loss: 0.5040\n",
            "259072/346575 [=====================>........] - ETA: 58s - loss: 0.5032\n",
            "260096/346575 [=====================>........] - ETA: 57s - loss: 0.5025\n",
            "261120/346575 [=====================>........] - ETA: 56s - loss: 0.5018\n",
            "262144/346575 [=====================>........] - ETA: 56s - loss: 0.5011\n",
            "263168/346575 [=====================>........] - ETA: 55s - loss: 0.5004\n",
            "264192/346575 [=====================>........] - ETA: 54s - loss: 0.4998\n",
            "265216/346575 [=====================>........] - ETA: 54s - loss: 0.4990\n",
            "266240/346575 [======================>.......] - ETA: 53s - loss: 0.4982\n",
            "267264/346575 [======================>.......] - ETA: 52s - loss: 0.4975\n",
            "268288/346575 [======================>.......] - ETA: 51s - loss: 0.4968\n",
            "269312/346575 [======================>.......] - ETA: 51s - loss: 0.4960\n",
            "270336/346575 [======================>.......] - ETA: 50s - loss: 0.4954\n",
            "271360/346575 [======================>.......] - ETA: 49s - loss: 0.4947\n",
            "272384/346575 [======================>.......] - ETA: 49s - loss: 0.4941\n",
            "273408/346575 [======================>.......] - ETA: 48s - loss: 0.4934\n",
            "274432/346575 [======================>.......] - ETA: 47s - loss: 0.4927\n",
            "275456/346575 [======================>.......] - ETA: 47s - loss: 0.4920\n",
            "276480/346575 [======================>.......] - ETA: 46s - loss: 0.4911\n",
            "277504/346575 [=======================>......] - ETA: 45s - loss: 0.4905\n",
            "278528/346575 [=======================>......] - ETA: 45s - loss: 0.4899\n",
            "279552/346575 [=======================>......] - ETA: 44s - loss: 0.4891\n",
            "280576/346575 [=======================>......] - ETA: 43s - loss: 0.4885\n",
            "281600/346575 [=======================>......] - ETA: 43s - loss: 0.4880\n",
            "282624/346575 [=======================>......] - ETA: 42s - loss: 0.4873\n",
            "283648/346575 [=======================>......] - ETA: 41s - loss: 0.4867\n",
            "284672/346575 [=======================>......] - ETA: 41s - loss: 0.4861\n",
            "285696/346575 [=======================>......] - ETA: 40s - loss: 0.4853\n",
            "286720/346575 [=======================>......] - ETA: 39s - loss: 0.4846\n",
            "287744/346575 [=======================>......] - ETA: 39s - loss: 0.4840\n",
            "288768/346575 [=======================>......] - ETA: 38s - loss: 0.4834\n",
            "289792/346575 [========================>.....] - ETA: 37s - loss: 0.4828\n",
            "290816/346575 [========================>.....] - ETA: 36s - loss: 0.4822\n",
            "291840/346575 [========================>.....] - ETA: 36s - loss: 0.4815\n",
            "292864/346575 [========================>.....] - ETA: 35s - loss: 0.4810\n",
            "293888/346575 [========================>.....] - ETA: 34s - loss: 0.4804\n",
            "294912/346575 [========================>.....] - ETA: 34s - loss: 0.4798\n",
            "295936/346575 [========================>.....] - ETA: 33s - loss: 0.4792\n",
            "296960/346575 [========================>.....] - ETA: 32s - loss: 0.4788\n",
            "297984/346575 [========================>.....] - ETA: 32s - loss: 0.4781\n",
            "299008/346575 [========================>.....] - ETA: 31s - loss: 0.4774\n",
            "300032/346575 [========================>.....] - ETA: 30s - loss: 0.4768\n",
            "301056/346575 [=========================>....] - ETA: 30s - loss: 0.4763\n",
            "302080/346575 [=========================>....] - ETA: 29s - loss: 0.4757\n",
            "303104/346575 [=========================>....] - ETA: 28s - loss: 0.4751\n",
            "304128/346575 [=========================>....] - ETA: 28s - loss: 0.4745\n",
            "305152/346575 [=========================>....] - ETA: 27s - loss: 0.4739\n",
            "306176/346575 [=========================>....] - ETA: 26s - loss: 0.4734\n",
            "307200/346575 [=========================>....] - ETA: 26s - loss: 0.4729\n",
            "308224/346575 [=========================>....] - ETA: 25s - loss: 0.4724\n",
            "309248/346575 [=========================>....] - ETA: 24s - loss: 0.4718\n",
            "310272/346575 [=========================>....] - ETA: 24s - loss: 0.4712\n",
            "311296/346575 [=========================>....] - ETA: 23s - loss: 0.4705\n",
            "312320/346575 [==========================>...] - ETA: 22s - loss: 0.4701\n",
            "313344/346575 [==========================>...] - ETA: 22s - loss: 0.4696\n",
            "314368/346575 [==========================>...] - ETA: 21s - loss: 0.4690\n",
            "315392/346575 [==========================>...] - ETA: 20s - loss: 0.4686\n",
            "316416/346575 [==========================>...] - ETA: 20s - loss: 0.4680\n",
            "317440/346575 [==========================>...] - ETA: 19s - loss: 0.4675\n",
            "318464/346575 [==========================>...] - ETA: 18s - loss: 0.4669\n",
            "319488/346575 [==========================>...] - ETA: 17s - loss: 0.4663\n",
            "320512/346575 [==========================>...] - ETA: 17s - loss: 0.4657\n",
            "321536/346575 [==========================>...] - ETA: 16s - loss: 0.4651\n",
            "322560/346575 [==========================>...] - ETA: 15s - loss: 0.4647\n",
            "323584/346575 [===========================>..] - ETA: 15s - loss: 0.4641\n",
            "324608/346575 [===========================>..] - ETA: 14s - loss: 0.4636\n",
            "325632/346575 [===========================>..] - ETA: 13s - loss: 0.4631\n",
            "326656/346575 [===========================>..] - ETA: 13s - loss: 0.4626\n",
            "327680/346575 [===========================>..] - ETA: 12s - loss: 0.4620\n",
            "328704/346575 [===========================>..] - ETA: 11s - loss: 0.4616\n",
            "329728/346575 [===========================>..] - ETA: 11s - loss: 0.4611\n",
            "330752/346575 [===========================>..] - ETA: 10s - loss: 0.4606\n",
            "331776/346575 [===========================>..] - ETA: 9s - loss: 0.4602 \n",
            "332800/346575 [===========================>..] - ETA: 9s - loss: 0.4597\n",
            "333824/346575 [===========================>..] - ETA: 8s - loss: 0.4591\n",
            "334848/346575 [===========================>..] - ETA: 7s - loss: 0.4587\n",
            "335872/346575 [============================>.] - ETA: 7s - loss: 0.4582\n",
            "336896/346575 [============================>.] - ETA: 6s - loss: 0.4576\n",
            "337920/346575 [============================>.] - ETA: 5s - loss: 0.4571\n",
            "338944/346575 [============================>.] - ETA: 5s - loss: 0.4567\n",
            "339968/346575 [============================>.] - ETA: 4s - loss: 0.4562\n",
            "340992/346575 [============================>.] - ETA: 3s - loss: 0.4557\n",
            "342016/346575 [============================>.] - ETA: 3s - loss: 0.4551\n",
            "343040/346575 [============================>.] - ETA: 2s - loss: 0.4547\n",
            "344064/346575 [============================>.] - ETA: 1s - loss: 0.4542\n",
            "345088/346575 [============================>.] - ETA: 0s - loss: 0.4536\n",
            "346112/346575 [============================>.] - ETA: 0s - loss: 0.4531\n",
            "346575/346575 [==============================] - 237s 683us/step - loss: 0.4529 - val_loss: 0.2941\n",
            "\n",
            " 1024/38509 [..............................] - ETA: 6s\n",
            " 2048/38509 [>.............................] - ETA: 6s\n",
            " 3072/38509 [=>............................] - ETA: 6s\n",
            " 4096/38509 [==>...........................] - ETA: 6s\n",
            " 5120/38509 [==>...........................] - ETA: 6s\n",
            " 6144/38509 [===>..........................] - ETA: 5s\n",
            " 7168/38509 [====>.........................] - ETA: 5s\n",
            " 8192/38509 [=====>........................] - ETA: 5s\n",
            " 9216/38509 [======>.......................] - ETA: 5s\n",
            "10240/38509 [======>.......................] - ETA: 5s\n",
            "11264/38509 [=======>......................] - ETA: 4s\n",
            "12288/38509 [========>.....................] - ETA: 4s\n",
            "13312/38509 [=========>....................] - ETA: 4s\n",
            "14336/38509 [==========>...................] - ETA: 4s\n",
            "15360/38509 [==========>...................] - ETA: 4s\n",
            "16384/38509 [===========>..................] - ETA: 3s\n",
            "17408/38509 [============>.................] - ETA: 3s\n",
            "18432/38509 [=============>................] - ETA: 3s\n",
            "19456/38509 [==============>...............] - ETA: 3s\n",
            "20480/38509 [==============>...............] - ETA: 3s\n",
            "21504/38509 [===============>..............] - ETA: 3s\n",
            "22528/38509 [================>.............] - ETA: 2s\n",
            "23552/38509 [=================>............] - ETA: 2s\n",
            "24576/38509 [==================>...........] - ETA: 2s\n",
            "25600/38509 [==================>...........] - ETA: 2s\n",
            "26624/38509 [===================>..........] - ETA: 2s\n",
            "27648/38509 [====================>.........] - ETA: 1s\n",
            "28672/38509 [=====================>........] - ETA: 1s\n",
            "29696/38509 [======================>.......] - ETA: 1s\n",
            "30720/38509 [======================>.......] - ETA: 1s\n",
            "31744/38509 [=======================>......] - ETA: 1s\n",
            "32768/38509 [========================>.....] - ETA: 1s\n",
            "33792/38509 [=========================>....] - ETA: 0s\n",
            "34816/38509 [==========================>...] - ETA: 0s\n",
            "35840/38509 [==========================>...] - ETA: 0s\n",
            "36864/38509 [===========================>..] - ETA: 0s\n",
            "37888/38509 [============================>.] - ETA: 0s\n",
            "38509/38509 [==============================] - 7s 180us/step\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n",
            "Raw test score: 0.29409317421468617\n",
            "Training accuracy: 0.9121376476162533\n",
            "Training confusion matrix:\n",
            "[[      0       0       0       0       0       0]\n",
            " [    321   15414  127473    5680       0      15]\n",
            " [    430    4813 3802005   11157       0      22]\n",
            " [    694    5975  162327   24864       0      16]\n",
            " [      0       0     137       2       0       0]\n",
            " [    443    3240   42933    4452       0     199]]\n",
            "Training results:\n",
            "(array([0.        , 0.5235378 , 0.91949696, 0.53870653, 0.        ,\n",
            "       0.78968254]), array([0.        , 0.10351705, 0.99569928, 0.12824692, 0.        ,\n",
            "       0.00388164]), array([0.        , 0.17285598, 0.95608214, 0.20717324, 0.        ,\n",
            "       0.00772531]), array([      0,  148903, 3818427,  193876,     139,   51267]))\n",
            "Testing accuracy: 0.9094315838217703\n",
            "Testing confusion matrix:\n",
            "[[     0      0      0      0      0      0]\n",
            " [    29   1760  14687    657      0      0]\n",
            " [    64    520 419701   1444      0      1]\n",
            " [    81    607  18447   2729      0      3]\n",
            " [     0      0     31      0      0      0]\n",
            " [    67    377   4764    467      0     18]]\n",
            "Testing results:\n",
            "(array([0.        , 0.53921569, 0.91711863, 0.51519728, 0.        ,\n",
            "       0.81818182]), array([0.        , 0.10272573, 0.99518886, 0.12479993, 0.        ,\n",
            "       0.00316178]), array([0.        , 0.1725744 , 0.95456013, 0.2009277 , 0.        ,\n",
            "       0.00629921]), array([     0,  17133, 421730,  21867,     31,   5693]))\n",
            "Output 'model/model_arch.json' didn't change. Skipping saving.\n",
            "Output 'model/metrics/test_score.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/train_acc.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/train_confusion.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/train_precision.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/train_recall.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/train_fbeta.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/train_support.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/test_acc.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/test_confusion.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/test_precision.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/test_recall.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/test_fbeta.json' doesn't use cache. Skipping saving.\n",
            "Output 'model/metrics/test_support.json' doesn't use cache. Skipping saving.\n",
            "Saving 'model/model_arch.json' to cache '.dvc/cache'.\n",
            "Saving 'model/model_params.json' to cache '.dvc/cache'.\n",
            "Saving 'model/model_weights.h5' to cache '.dvc/cache'.\n",
            "Saving 'model/learn-stdout.txt' to cache '.dvc/cache'.\n",
            "Saving information to 'python/learn.dvc'.\n",
            "\n",
            "To track the changes with git run:\n",
            "\n",
            "\tgit add python/learn.dvc\n",
            "\u001b[0m\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8YKSubrKt_Sf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Commit the results of the experiment"
      ]
    },
    {
      "metadata": {
        "id": "cR74zaPepSuf",
        "colab_type": "code",
        "outputId": "1163f534-ee75-464c-9fa6-40e179612b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "# TODO: Git commit, dvc metrics --all, dvc push, git push\n",
        "!git add .\n",
        "!git status\n",
        "!git commit -m \"Results of {experiment_name}\"\n",
        "!git push --set-upstream origin {experiment_name}\n",
        "!dvc push"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On branch second-experiment\n",
            "Your branch is ahead of 'origin/master' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mmodified:   model/metrics/test_acc.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/test_confusion.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/test_fbeta.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/test_precision.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/test_recall.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/test_score.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/test_support.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/train_acc.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/train_confusion.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/train_fbeta.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/train_precision.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/train_recall.json\u001b[m\n",
            "\t\u001b[32mmodified:   model/metrics/train_support.json\u001b[m\n",
            "\t\u001b[32mmodified:   python/learn.dvc\u001b[m\n",
            "\n",
            "[second-experiment cf2de1d] Results of second-experiment\n",
            " 14 files changed, 32 insertions(+), 31 deletions(-)\n",
            "Counting objects: 23, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (20/20), done.\n",
            "Writing objects: 100% (23/23), 2.85 KiB | 2.85 MiB/s, done.\n",
            "Total 23 (delta 5), reused 0 (delta 0)\n",
            "To https://dagshub.com/Guy/uri_nlp_ner_workshop.git\n",
            " * [new branch]      second-experiment -> second-experiment\n",
            "Branch 'second-experiment' set up to track remote branch 'second-experiment' from 'origin'.\n",
            "Preparing to upload data to '/content/gdrive/My Drive/nlp-workshop-dvc-cache'\n",
            "Preparing to collect status from /content/gdrive/My Drive/nlp-workshop-dvc-cache\n",
            "\u001b[K[##############################] 100% Collecting information\n",
            "\u001b[K[##############################] 100% Analysing status.\n",
            "\u001b[K(1/3): [##############################] 100% ../model/learn-stdout.txt\n",
            "\u001b[K(2/3): [##############################] 100% ../model/model_params.json\n",
            "\u001b[K(3/3): [##############################] 100% ../model/model_weights.h5\n",
            "\u001b[0m\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cjseaLxiwZ0V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiments Overview\n",
        "Compare the achieved results across your different experiments, each one saved in a git branch.\n",
        "\n",
        "For easier comparison of your different experiments, we reccomend you push this repo to DAGsHub and use the \"Branches\" view, e.g.: https://dagshub.com/Guy/uri_nlp_ner_workshop/branches"
      ]
    },
    {
      "metadata": {
        "id": "_7s2RJfCssuU",
        "colab_type": "code",
        "outputId": "2cf5624d-7a39-48fd-8047-889abad6ad51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "!dvc metrics show model/metrics/test_acc.json --all-branches"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first-experiment:\n",
            "\tmodel/metrics/test_acc.json: [0.9120653399696448]\n",
            "master:\n",
            "\tmodel/metrics/test_acc.json: [0.9111092117352935]\n",
            "second-experiment:\n",
            "\tmodel/metrics/test_acc.json: [0.9094315838217703]\n",
            "\u001b[0m\u001b[0m"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GM1L3YqRw1IP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Submit results\n",
        "After you decide which branch you want, submit the results of that branch"
      ]
    },
    {
      "metadata": {
        "id": "hf6aIupGwm7U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "best_experiment_name = \"???\"\n",
        "!git checkout {best_experiment_name}\n",
        "!dvc pull\n",
        "!dvc checkout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C3ERUn-SG2lM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import python.gorenml as gorenml\n",
        "test_submission = gorenml.Submission(best_experiment_name, model_folder=\"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kvx6OUSHHbk",
        "colab_type": "code",
        "outputId": "cab36999-34a1-4406-b136-47ce07647f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_submission.submit(test_folder='data/test_txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 382/382 [02:40<00:00,  3.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46136575166199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    }
  ]
}